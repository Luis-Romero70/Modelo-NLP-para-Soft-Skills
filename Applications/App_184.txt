Responsibilities 
Develop reliable, reusable, automated, and streamlined ETL code. Analyze and organize raw data from multiple sources to produce requested or required data elements. 
Design table structures and ETL to build performant data solutions that are scalable in a fast-growing data ecosystem 
Develop procedures and measures to ensure the accuracy of data reconciliation. Integrate automatic audit, balancing, and controls. 
Building reports/dashboards in Power BI or Snowflake Build Data Catalog 
Assist with production, including being available and providing technical support to end-users and IT workers. 
Explore data and assist business partners with intricate data analysis and impromptu inquiries. 
Performed analysis, design, and implementation of application and technical architecture and confirmed that the BI product fulfilled the requirements. 
Collaborate with data management and database specialists to identify and document issues. 
Create new reporting applications to ensure that the underlying data source can meet reporting requirements. 
Adhere to all requirements for managing metadata and ensuring data quality by overseeing quality management reviews. 
Understand the relationships and interdependencies between data warehouse and business intelligence tools and architectures; address problems in this area based on this understanding. 
Proactively seek, establish, and cultivate ties with business process owners and colleagues. 
Identify changes in project scope or workload that could result in exceeding the budget or missing the delivery date and create and implement contingency measures. 


Qualifications 
Degree in Computer Science, IT, or a similar field; a masterâ€™s is a plus 
Expertise in DBT, SQL, and Python 
Preferred five or more years of experience with SQL, Data Modeling, and Dimensional Modeling 
Technical expertise in data models, data mining, and segmentation techniques 
Experience with Microsoft Dataverse, Datawarehouse, Data Factory, Synapse, PowerApps, Project Central 
Hands-on experience building large-scale data pipelines using tools such as Apache Airflow, Azure Data Factory, or other equivalent technology, 
Minimum two years experience with big data technologies such as Apache Spark (or Databricks), Hadoop, or equivalent technology 
Minimum two years experience implementing data solutions in the public cloud (Azure, AWS, or GCP) 
Hands-on experience with NoSQL technologies such as Cassandra, CouchDB, MongoDB 
Strong teamwork, analytical nature, self-motivation, and excellent written and oral communication skills. 
Ability to maintain own workflow and meet deadlines while managing parallel project deliverables with minimal direction 
Ability to create compelling visual models to enable collective understanding of processes, data flow, user interaction, and others as needed. 
Hands-on experience with data model design 
Excellent numerical and analytical skills