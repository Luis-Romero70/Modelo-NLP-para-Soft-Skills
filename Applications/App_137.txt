Intermediate Data Engineer
Remote - Canada | Contract

Our Canadian client is a fast-growing Snowflake Services Partner working across North America, looking for an Intermediate Data Engineer to join their team for a long-term contract opportunity. It’s a fully remote engagement, successful candidates can be located anywhere in Latin America, but you'll need to be available for significant overlap with PST time zone. You'll need to have good English communication skills (at least B2 or higher) for this role.

This is a great opportunity for an up-and-coming data engineer interested in training, certifications, and gaining international experience and English language proficiency. We want to hear from you if you are ready to break boundaries in the data world, work with innovative clients, and love to continuously research and learn new technologies like DBT, Airbyte and Airflow, or Terraform.

What you’ll get to do:

Interact with internal technical teams and potentially purely business-oriented clients: analyze customer data requirements and participate in crafting suitable solutions.
Collect data from various sources, such as databases, APIs, and logs, and integrating it into a unified data ecosystem.
Perform data cleaning, structuring, and transformation tasks to ensure that raw data is in a usable format for analysis.
Manage databases, including creating and maintaining database structures, optimizing query performance, and ensuring data consistency and integrity.
Work on developing and maintaining ETL pipelines, which involve extracting data from source systems, transforming it into the desired format, and loading it into a destination database or data warehouse.
Monitor data quality, implement data validation checks, and troubleshoot data-related issues to maintain data accuracy and reliability.

Requirements:

Passion for data and engineering excellence
Strong command of SQL, hands-on experience in writing complex queries, optimizing database performance, and designing efficient database schemas.
Proven skills in creating and maintaining data models, including conceptual, logical, and physical models.
Intermediate level ETL experience
Familiarity with data warehousing concepts and have hands-on experience with data warehousing solutions such as Amazon Redshift, Google BigQuery, or Snowflake (preferred)
Intermediate level hands-on experience building data solutions using a variety of traditional and/or big data technologies
Software engineering experience is an asset

What we work with:

Modern Data Workflows (DBT, Airflow, Prefect, Dagster)
CI/CD (Gitlab, GitHub Actions, Jenkins)
Programming Languages (Python, JavaScript, Kotlin or Java)
Big Data Platforms (Apache Spark, Presto, Amazon EMR)
Cloud Data Warehouses (Snowflake Data Cloud, Google BigQuery, DataBricks Lakehouse, Azure Synapse)
Databases (SQL Server, PostgreSQL, MySql, Oracle, Vertica)
Container Management Systems (Kubernetes)
Visual Analytics (Tableau, Looker, PowerBI)
Artificial Intelligence / Machine Learning (Amazon Sagemaker, Azure ML Studio)
Streaming Data Ingestion and Analytics (Amazon Kinesis, Apache Kafka)

Of Note:

You will be required to provide you own laptop, basic software tools, and cell phone.
Completed background checks will be required prior to the start date if you are selected as a winning candidate.
You will be required to sign our standard non-moonlighting agreement prior to the start date if you are selected as a winning candidate.
As a winning candidate you will be required to disclose your engagement with DevEngine as a primary client on your professional LinkedIn profile.

While we strive to respond to all applicants, please understand that due to the high volume of applications we receive, it may not be feasible to provide individual feedback or responses to every candidate. Rest assured that your application will be carefully reviewed and considered. We appreciate your understanding and interest in joining our team.