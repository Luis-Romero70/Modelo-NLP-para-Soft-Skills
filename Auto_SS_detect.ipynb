{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Automated Soft Skills detection in IT Job Applications</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocesar los datos de entrenamiento (el archivo CSV adjunto):**<br>\n",
    "1. Limpiar y normalizar el texto de las postulaciones (quitar signos de puntuación, convertir a minúsculas, etc.)\n",
    "2. Extraer las soft skills asociadas a cada postulación para tener las etiquetas de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    buscamos un data scientist altamente motivado ...\n",
      "1    como data scientist en nuestro equipo particip...\n",
      "2    estamos en busca de un data scientist apasiona...\n",
      "3    nete a nuestro equipo como data scientist y co...\n",
      "4    estamos contratando un data scientist con habi...\n",
      "Name: text, dtype: object\n",
      "0    [comunicación, trabajo en equipo, habilidades ...\n",
      "1                                       [comunicación]\n",
      "2             [resolución de problemas, adaptabilidad]\n",
      "3                    [comunicación, trabajo en equipo]\n",
      "4                             [habilidades analíticas]\n",
      "Name: soft_skills, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Leer el archivo CSV\n",
    "df = pd.read_csv('label_data.csv', sep=';')\n",
    "\n",
    "# Normalizar texto \n",
    "df['text'] = df['text'].str.lower() # convertir a minúsculas\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'[^a-zA-z0-9\\s]', '', x)) # remover puntuación\n",
    "\n",
    "# Extraer las labels de soft skills\n",
    "df['soft_skills'] = df['soft_skills'].str.lower()\n",
    "# df['soft_skills'] = df['soft_skills'].str.split(',')\n",
    "df['soft_skills'] = df['soft_skills'].apply(lambda x: eval(x) if pd.notna(x) else [])\n",
    "\n",
    "\n",
    "# Now check for cells with empty lists\n",
    "empty_lists = df['soft_skills'].apply(lambda x: len(x) == 0)\n",
    "df_filtered = df[~empty_lists]\n",
    "\n",
    "# Imprimir algunos ejemplos\n",
    "print(df['text'].head())\n",
    "print(df['soft_skills'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Hay celdas vacías en la columna \"soft_skills\"? False\n"
     ]
    }
   ],
   "source": [
    "# Verificar si hay celdas vacías en la columna 'soft_skills'\n",
    "celdas_vacias = df['soft_skills'].isnull().any()\n",
    "\n",
    "# Imprimir el resultado\n",
    "print(f'¿Hay celdas vacías en la columna \"soft_skills\"? {celdas_vacias}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Asegúrate de descargar los recursos necesarios para nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 5 is present in all training examples.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 6 is present in all training examples.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 12 is present in all training examples.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 25 is present in all training examples.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 27 is present in all training examples.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Importar librerías\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Leer el archivo CSV\n",
    "df = pd.read_csv('label_data.csv', sep=';')\n",
    "\n",
    "# Eliminar postulaciones sin soft skills asociadas\n",
    "df['soft_skills'] = df['soft_skills'].apply(lambda x: eval(x) if pd.notna(x) else [])\n",
    "df = df[df['soft_skills'].apply(len) > 0]\n",
    "\n",
    "# Normalizar texto \n",
    "df['text'] = df['text'].str.lower()  # convertir a minúsculas\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'[^a-zA-z0-9\\s]', '', x))  # remover puntuación\n",
    "\n",
    "# Tokenizar el texto\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize_text) \n",
    "\n",
    "# Crear vectores de texto con TF-IDF\n",
    "X = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# Convertir etiquetas a formato binario\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_bin = mlb.fit_transform(df['soft_skills'])\n",
    "\n",
    "# Dividir datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_bin, test_size=0.2)\n",
    "\n",
    "# Entrenar un modelo Naive Bayes con OneVsRest\n",
    "nb = MultinomialNB()\n",
    "model = OneVsRestClassifier(nb)\n",
    "\n",
    "# Entrenar modelo\n",
    "model.fit(X_train, y_train) \n",
    "\n",
    "# Evaluar performance\n",
    "print(model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "        0, 0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puntuaciones de Validación Cruzada: [0. 0. 0. 0. 0.]\n",
      "Precisión Promedio: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 5 is present in all training examples.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 6 is present in all training examples.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 12 is present in all training examples.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 25 is present in all training examples.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 27 is present in all training examples.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 5 is present in all training examples.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 6 is present in all training examples.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 12 is present in all training examples.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 22 is present in all training examples.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 25 is present in all training examples.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 27 is present in all training examples.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 5 is present in all training examples.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 6 is present in all training examples.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 12 is present in all training examples.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 14 is present in all training examples.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 25 is present in all training examples.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 27 is present in all training examples.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 5 is present in all training examples.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 6 is present in all training examples.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 12 is present in all training examples.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 16 is present in all training examples.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 25 is present in all training examples.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 27 is present in all training examples.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 5 is present in all training examples.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 6 is present in all training examples.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 11 is present in all training examples.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 12 is present in all training examples.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 15 is present in all training examples.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 25 is present in all training examples.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\multiclass.py:84: UserWarning: Label not 27 is present in all training examples.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Utilizar validación cruzada\n",
    "scores = cross_val_score(model, X_train, y_train, cv=5)  # Puedes ajustar el número de pliegues (folds) según sea necesario\n",
    "\n",
    "# Imprimir las puntuaciones de validación cruzada\n",
    "print(\"Puntuaciones de Validación Cruzada:\", scores)\n",
    "print(\"Precisión Promedio:\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Ajustar el vectorizador TF-IDF en los datos de entrenamiento\u001b[39;00m\n\u001b[0;32m     21\u001b[0m tfidf \u001b[38;5;241m=\u001b[39m TfidfVectorizer(tokenizer\u001b[38;5;241m=\u001b[39mtokenize_text)\n\u001b[1;32m---> 22\u001b[0m X_train_tfidf \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m X_test_tfidf \u001b[38;5;241m=\u001b[39m tfidf\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Convertir los documentos a minúsculas después de la vectorización\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2126\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2121\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2122\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2123\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2124\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2125\u001b[0m )\n\u001b[1;32m-> 2126\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2128\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2129\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1383\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1375\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1376\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1377\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1378\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1379\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1380\u001b[0m             )\n\u001b[0;32m   1381\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1383\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1386\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1270\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1269\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1270\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1271\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1272\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:68\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[1;32m---> 68\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\scipy\\sparse\\_base.py:771\u001b[0m, in \u001b[0;36mspmatrix.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetnnz()\n\u001b[0;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 771\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(attr \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# # Asegúrate de tener la función tokenize_text definida\n",
    "# def tokenize_text(text):\n",
    "#     # Implementa la tokenización según sea necesario\n",
    "#     return text.split()  # Ejemplo simple, tokeniza por espacios en blanco\n",
    "\n",
    "# Convertir etiquetas de clase a formato binario\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train_encoded = mlb.fit_transform(y_train)\n",
    "y_test_encoded = mlb.transform(y_test)\n",
    "\n",
    "# Ajustar el vectorizador TF-IDF en los datos de entrenamiento\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize_text)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Convertir los documentos a minúsculas después de la vectorización\n",
    "X_train_lower = [doc.lower() for doc in X_train]\n",
    "X_test_lower = [doc.lower() for doc in X_test]\n",
    "\n",
    "# Importar modelos a probar\n",
    "models = [\n",
    "    ('Naive Bayes', MultinomialNB()),\n",
    "    ('Regresión Logística', LogisticRegression()), \n",
    "    ('SVM', SVC()),\n",
    "    ('Red Neuronal', MLPClassifier())\n",
    "]\n",
    "\n",
    "# Evaluar cada modelo\n",
    "results = []\n",
    "for name, model in models:\n",
    "    # Ajustar el modelo al conjunto de entrenamiento\n",
    "    model.fit(X_train_tfidf, y_train_encoded)\n",
    "    # Predecir con el conjunto de prueba\n",
    "    pred = model.predict(X_test_tfidf)\n",
    "    # Evaluar métrica\n",
    "    accuracy = accuracy_score(y_test_encoded, pred)\n",
    "    # Guardar resultado \n",
    "    results.append((name, accuracy))\n",
    "\n",
    "# Ordenar resultados  \n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print('Performance de modelos:')\n",
    "for name, accuracy in results:\n",
    "    print(f'{name}: {accuracy:.2%}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'apply'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m tfidf \u001b[38;5;241m=\u001b[39m TfidfVectorizer(tokenizer\u001b[38;5;241m=\u001b[39mtokenize_text)  \n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Convertir etiquetas de clase a listas\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m y_train \u001b[38;5;241m=\u001b[39m \u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28meval\u001b[39m(x) \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mnotna(x) \u001b[38;5;28;01melse\u001b[39;00m [])\n\u001b[0;32m     16\u001b[0m y_test \u001b[38;5;241m=\u001b[39m y_test\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28meval\u001b[39m(x) \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mnotna(x) \u001b[38;5;28;01melse\u001b[39;00m [])\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Codificar etiquetas con LabelEncoder\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'apply'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Asegúrate de tener la función tokenize_text definida\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize_text)  \n",
    "\n",
    "# Convertir etiquetas de clase a listas\n",
    "y_train = y_train.apply(lambda x: eval(x) if pd.notna(x) else [])\n",
    "y_test = y_test.apply(lambda x: eval(x) if pd.notna(x) else [])\n",
    "\n",
    "# Codificar etiquetas con LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Convertir etiquetas de habilidades blandas a vectores binarios\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train_encoded = mlb.fit_transform(y_train)\n",
    "y_test_encoded = mlb.transform(y_test)\n",
    "\n",
    "# Ajustar el vectorizador TF-IDF en los datos de entrenamiento\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "\n",
    "# Importar modelos a probar\n",
    "models = [\n",
    "    ('Naive Bayes', MultinomialNB()),\n",
    "    ('Regresión Logística', LogisticRegression()), \n",
    "    ('SVM', SVC()),\n",
    "    ('Red Neuronal', MLPClassifier())\n",
    "]\n",
    "\n",
    "# Evaluar cada modelo\n",
    "results = []\n",
    "for name, model in models:\n",
    "    if name in ['SVM', 'Regresión Logística']:\n",
    "        # Ajustar el modelo al conjunto de entrenamiento\n",
    "        model.fit(X_train_tfidf, y_train_encoded)\n",
    "        # No es necesario ajustar el modelo aquí, ya que se ajustó previamente en el código\n",
    "        pred = model.predict(X_test_tfidf)\n",
    "    else:\n",
    "        # Ajustar el modelo al conjunto de entrenamiento\n",
    "        model.fit(X_train_tfidf, y_train_encoded)\n",
    "        # Transformar las etiquetas de prueba sin ajustar\n",
    "        pred = model.predict(X_test_tfidf)\n",
    "    # Evaluar métrica\n",
    "    accuracy = accuracy_score(y_test_encoded, pred)\n",
    "    # Guardar resultado \n",
    "    results.append((name, accuracy))\n",
    "\n",
    "# Ordenar resultados  \n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print('Performance de modelos:')\n",
    "for name, accuracy in results:\n",
    "    print(f'{name}: {accuracy:.2%}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Ajustar el vectorizador TF-IDF en los datos de entrenamiento\u001b[39;00m\n\u001b[0;32m     11\u001b[0m tfidf \u001b[38;5;241m=\u001b[39m TfidfVectorizer(tokenizer\u001b[38;5;241m=\u001b[39mtokenize_text)\n\u001b[1;32m---> 12\u001b[0m X_train_tfidf \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m X_test_tfidf \u001b[38;5;241m=\u001b[39m tfidf\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Convertir etiquetas de clase a listas\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2126\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2121\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2122\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2123\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2124\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2125\u001b[0m )\n\u001b[1;32m-> 2126\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2128\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2129\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1383\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1375\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1376\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1377\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1378\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1379\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1380\u001b[0m             )\n\u001b[0;32m   1381\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1383\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1386\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1270\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1269\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1270\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1271\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1272\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:68\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[1;32m---> 68\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\scipy\\sparse\\_base.py:771\u001b[0m, in \u001b[0;36mspmatrix.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetnnz()\n\u001b[0;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 771\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(attr \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "\n",
    "# Ajustar el vectorizador TF-IDF en los datos de entrenamiento\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize_text)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Convertir etiquetas de clase a listas\n",
    "y_train = y_train.apply(lambda x: eval(x) if pd.notna(x) else [])\n",
    "y_test = y_test.apply(lambda x: eval(x) if pd.notna(x) else [])\n",
    "\n",
    "# Codificar etiquetas con MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train_encoded = mlb.fit_transform(y_train)\n",
    "y_test_encoded = mlb.transform(y_test)\n",
    "\n",
    "# Importar modelos a probar\n",
    "models = [\n",
    "    ('Naive Bayes', MultinomialNB()),\n",
    "    ('Regresión Logística', LogisticRegression()), \n",
    "    ('SVM', SVC()),\n",
    "    ('Red Neuronal', MLPClassifier())\n",
    "]\n",
    "\n",
    "# Evaluar cada modelo\n",
    "results = []\n",
    "for name, model in models:\n",
    "    if name in ['SVM', 'Regresión Logística']:\n",
    "        # Ajustar el modelo al conjunto de entrenamiento\n",
    "        model.fit(X_train_tfidf, y_train_encoded)\n",
    "        # No es necesario ajustar el modelo aquí, ya que se ajustó previamente en el código\n",
    "        pred = model.predict(X_test_tfidf)\n",
    "    else:\n",
    "        # Ajustar el modelo al conjunto de entrenamiento\n",
    "        model.fit(X_train_tfidf, y_train_encoded)\n",
    "        # Transformar las etiquetas de prueba sin ajustar\n",
    "        pred = model.predict(X_test_tfidf)\n",
    "    # Evaluar métrica\n",
    "    accuracy = accuracy_score(y_test_encoded, pred)\n",
    "    # Guardar resultado \n",
    "    results.append((name, accuracy))\n",
    "\n",
    "# Ordenar resultados  \n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print('Performance de modelos:')\n",
    "for name, accuracy in results:\n",
    "    print(f'{name}: {accuracy:.2%}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
